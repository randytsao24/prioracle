{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train.drop(train[train.price < 3.0].index)\n",
    "# train = train.drop(train[train.price > 2000.0].index)\n",
    "\n",
    "# brands = train.brand_name.dropna().unique()\n",
    "\n",
    "# def get_brand_from_item_name(brand, item_name):\n",
    "#     if pd.notnull(brand) or not any(x in item_name for x in brands):\n",
    "#         return brand\n",
    "#     else:\n",
    "#         for x in brands:\n",
    "#             if x in item_name:\n",
    "#                 return x\n",
    "            \n",
    "# train['brand_name'] = train.apply(\n",
    "#     lambda row: get_brand_from_item_name(row['brand_name'], row['name']), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# def remove_brand_from_item_name(item_name):\n",
    "#     for brand in brands:\n",
    "#         if brand + ' ' in item_name:\n",
    "#             return item_name.replace(brand + ' ', '')\n",
    "#         elif ' ' + brand in item_name:\n",
    "#             return item_name.replace(' ' + brand, '')\n",
    "#         elif ' ' + brand + ' ' in item_name:\n",
    "#             return item_name.replace(' ' + brand + ' ', '')\n",
    "        \n",
    "# # train['name'] = train.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table('train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.log1p(train['price'])\n",
    "train['category_name'] = train['category_name'].fillna('Other').astype(str)\n",
    "train['brand_name'] = train['brand_name'].fillna('missing').astype(str)\n",
    "train['shipping'] = train['shipping'].astype(str)\n",
    "train['item_condition_id'] = train['item_condition_id'].astype(str)\n",
    "train['item_description'] = train['item_description'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 3s, sys: 4min 45s, total: 14min 48s\n",
      "Wall time: 3h 14min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# we need a custom pre-processor to extract correct field,\n",
    "# but want to also use default scikit-learn preprocessing (e.g. lowercasing)\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "def build_preprocessor(field):\n",
    "    field_idx = list(train.drop(['train_id', 'price'], axis=1).columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50000,\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "    ('category_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('category_name'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=100000,\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "\n",
    "X_train = vectorizer.fit_transform(train.drop(['train_id', 'price'], axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03c__main__\\nbuild_preprocessor\\nq\\x00.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dumps(build_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_preprocessor.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1b9fbd2d7c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpkl_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vectorizer.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_preprocessor.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "pkl_filename = 'vectorizer.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_preprocessor.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3263875ad311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectorizer.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_preprocessor.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "    pickle.dump(vectorizer, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 s, sys: 2.71 s, total: 53.9 s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "# cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# for train_ids, valid_ids in cv.split(X_train):\n",
    "#     model = Ridge(\n",
    "#         solver='auto',\n",
    "#         fit_intercept=True,\n",
    "#         alpha=0.5,\n",
    "#         max_iter=100\n",
    "#         normalize=False,\n",
    "#         tol=0.05\n",
    "#     )\n",
    "#     model.fit(X_train[train_ids], y_train[train_ids])\n",
    "#     y_pred_valid = model.predict(X_train[valid_ids])\n",
    "#     rmsle = get_rmsle(y_predict_valid, y_train[valid_ids])\n",
    "#     print(f'valid rmsle: {rmsle.5f}')\n",
    "#     break\n",
    "\n",
    "model = linear_model.Ridge(\n",
    "    solver='auto',\n",
    "    fit_intercept=True,\n",
    "    alpha=0.5,\n",
    "    max_iter=100,\n",
    "    normalize=False,\n",
    "    tol=0.05\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4233332155616298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmsle(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pkl_filename = 'model.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_preprocessor.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b9fbd2d7c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpkl_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vectorizer.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_preprocessor.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "pkl_filename = 'vectorizer.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
